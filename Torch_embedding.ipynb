{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Twittern Airlines sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import copy\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new dataframe with labels, text, sentiment label only\n",
    "df= pd.read_csv(\"text_air.csv\",index_col=[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token= data.Field(tokenize='spacy')\n",
    "sentiment_label = data.LabelField(dtype=torch.float)\n",
    "\n",
    "df = data.TabularDataset(path='text_air.csv',\n",
    "                        format='csv', \n",
    "                        fields=[('Unnamed', None),(\"text\",text_token),\\\n",
    "                                (\"airline_sentiment\",sentiment_label)],\n",
    "                        skip_header=True)\n",
    "\n",
    "########## split the data into train, valid and test sample\n",
    "train_df, test_data = df.split(random_state=random.seed(19))\n",
    "train_data, valid_data = train_df.split(random_state=random.seed(10))\n",
    "\n",
    "###########Building the vocab\n",
    "from torchtext import vocab\n",
    "text_token.build_vocab(train_data, valid_data, max_size=100000, vectors=\"glove.6B.100d\")\n",
    "sentiment_label.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the average and max pool 2d. The word is embedded into a 2-dimensional grid, \n",
    "where the words are along one axis and the dimensions of the word embeddings are along the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embednet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim*2, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        avg_pooled = F.max_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        max_pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        x=self.fc(torch.cat([avg_pooled,max_pooled],dim=1))    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_token.vocab)\n",
    "embedding_dim = 120\n",
    "n_out = 3\n",
    "step_size=0.0005\n",
    "decay=.9\n",
    "model = Embednet(vocab_size, embedding_dim, n_out)\n",
    "\n",
    "#pretrained_embeddings = text_token.vocab.vectors\n",
    "#model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=step_size)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=decay)\n",
    "criterion=F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BucketIterator returns a batch object\n",
    "train_batch, valid_batch, test_batch = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=128,sort_within_batch=False,sort=False,\n",
    "    device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state_dict = copy.deepcopy(model.state_dict()) ## saves the best model weights\n",
    "best_score = [-np.inf]\n",
    "def callback(epoch):\n",
    "    model.eval()\n",
    "\n",
    "    offset = 0; valid_loss = 0\n",
    "    valid_acc=0\n",
    "    for entry in valid_batch:\n",
    "        offset+=1\n",
    "        pred = model(entry.text).squeeze(1)\n",
    "        entry.airline_sentiment = entry.airline_sentiment.type(torch.LongTensor)\n",
    "        loss = criterion(pred, entry.airline_sentiment)\n",
    "        valid_loss += loss.item()\n",
    "        pred = pred.data.max(1)[1]\n",
    "        valid_acc += float(pred.eq(entry.airline_sentiment.data).sum())\n",
    "\n",
    "    print(\"valid_loss:\", valid_loss / len(valid_batch))\n",
    "    print(\"valid_accuracy:\", valid_acc / len(valid_batch))\n",
    "    \n",
    "    if (valid_acc / len(valid_batch)) > best_score[0]:\n",
    "       best_score[0]=valid_acc / len(valid_batch)\n",
    "       best_model_state_dict[0] = copy.deepcopy(model.state_dict())\n",
    "       with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(best_model_state_dict[0], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch started\n",
      "---------------------------\n",
      "epoch = 0\n",
      "valid_loss: 0.8862094378471375\n",
      "valid_accuracy: 77.04\n",
      "Epoch took 1.9012601375579834 seconds\n",
      "train_loss: 0.9158253523341396\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 1\n",
      "valid_loss: 0.8570388913154602\n",
      "valid_accuracy: 78.12\n",
      "Epoch took 1.8309900760650635 seconds\n",
      "train_loss: 0.8361703805756151\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 2\n",
      "valid_loss: 0.8478151321411133\n",
      "valid_accuracy: 79.04\n",
      "Epoch took 1.7695538997650146 seconds\n",
      "train_loss: 0.8104155492364314\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 3\n",
      "valid_loss: 0.8269103264808655\n",
      "valid_accuracy: 79.44\n",
      "Epoch took 1.7865290641784668 seconds\n",
      "train_loss: 0.7892982029078299\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 4\n",
      "valid_loss: 0.8175441217422486\n",
      "valid_accuracy: 80.32\n",
      "Epoch took 1.795170783996582 seconds\n",
      "train_loss: 0.7791543111466525\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 5\n",
      "valid_loss: 0.8037129354476928\n",
      "valid_accuracy: 81.12\n",
      "Epoch took 1.7632520198822021 seconds\n",
      "train_loss: 0.7710576726679217\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 6\n",
      "valid_loss: 0.7934429287910462\n",
      "valid_accuracy: 81.48\n",
      "Epoch took 1.7751789093017578 seconds\n",
      "train_loss: 0.7508376533525032\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 7\n",
      "valid_loss: 0.7894320726394654\n",
      "valid_accuracy: 81.52\n",
      "Epoch took 1.765105962753296 seconds\n",
      "train_loss: 0.7385417118407133\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 8\n",
      "valid_loss: 0.7823996806144714\n",
      "valid_accuracy: 83.56\n",
      "Epoch took 1.8150279521942139 seconds\n",
      "train_loss: 0.7265633825670209\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 9\n",
      "valid_loss: 0.7706645154953002\n",
      "valid_accuracy: 83.56\n",
      "Epoch took 1.797590970993042 seconds\n",
      "train_loss: 0.7252938203644335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "iteration =1\n",
    "import time\n",
    "train_loss_epoch=[]\n",
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    print(\"epoch started\")\n",
    "    print(\"---------------------------\")\n",
    "    print(\"epoch = %d\" % epoch)\n",
    "    #print(\"step_size = %.4f\" % step_size)\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for batch in train_batch:\n",
    "        iteration += 1\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred= model(batch.text).squeeze(1)\n",
    "        batch.airline_sentiment = batch.airline_sentiment.type(torch.LongTensor)\n",
    "        loss = criterion(pred, batch.airline_sentiment)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        t1 = time.time() \n",
    "    callback(epoch)\n",
    "    print(\"Epoch took {} seconds\".format(t1-t0)) \n",
    "    print(\"train_loss:\",train_loss / len(train_batch))\n",
    "    train_loss_epoch.append([epoch,train_loss / len(train_batch)])\n",
    "    print()\n",
    "    scheduler.step()\n",
    "    step_size = step_size * decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pt', 'rb') as f:\n",
    "     state_dict = torch.load(f,map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embednet(\n",
       "  (embedding): Embedding(15475, 120)\n",
       "  (fc): Linear(in_features=240, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Embednet(vocab_size, embedding_dim, n_out)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We obtain test accuracy: 86.02857142857142\n"
     ]
    }
   ],
   "source": [
    "test_acc=0.0\n",
    "for entry in test_batch:\n",
    "    pred = model(entry.text).squeeze(1)\n",
    "    entry.airline_sentiment = entry.airline_sentiment.type(torch.LongTensor)\n",
    "    pred = pred.data.max(1)[1]\n",
    "    test_acc += float(pred.eq(entry.airline_sentiment.data).sum())\n",
    "print(\" We obtain test accuracy:\",test_acc / len(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
