{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Twittern Airlines sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import copy\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new dataframe with labels, text, sentiment label only\n",
    "df= pd.read_csv(\"text_air.csv\",index_col=[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token= data.Field(tokenize='spacy')\n",
    "sentiment_label = data.LabelField(dtype=torch.float)\n",
    "\n",
    "df = data.TabularDataset(path='text_air.csv',\n",
    "                        format='csv', \n",
    "                        fields=[('Unnamed', None),(\"text\",text_token),\\\n",
    "                                (\"airline_sentiment\",sentiment_label)],\n",
    "                        skip_header=True)\n",
    "\n",
    "########## split the data into train, valid and test sample\n",
    "train_df, test_data = df.split(random_state=random.seed(19))\n",
    "train_data, valid_data = train_df.split(random_state=random.seed(10))\n",
    "\n",
    "###########Building the vocab\n",
    "from torchtext import vocab\n",
    "text_token.build_vocab(train_data, valid_data, max_size=100000, vectors=\"glove.6B.100d\")\n",
    "sentiment_label.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the average and max pool 2d. The word is embedded into a 2-dimensional grid, \n",
    "where the words are along one axis and the dimensions of the word embeddings are along the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embednet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim*2, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        avg_pooled = F.max_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        max_pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        x=self.fc(torch.cat([avg_pooled,max_pooled],dim=1))    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_token.vocab)\n",
    "embedding_dim = 120\n",
    "n_out = 3\n",
    "step_size=0.0005\n",
    "decay=.9\n",
    "model = Embednet(vocab_size, embedding_dim, n_out)\n",
    "\n",
    "#pretrained_embeddings = text_token.vocab.vectors\n",
    "#model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=step_size)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=decay)\n",
    "criterion=F.cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BucketIterator returns a batch object\n",
    "train_batch, valid_batch, test_batch = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=128,sort_within_batch=False,sort=False,\n",
    "    device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state_dict = copy.deepcopy(model.state_dict()) ## saves the best model weights\n",
    "def callback(epoch):\n",
    "    best_score = -np.inf   ### keeps track of valid accuracy\n",
    "    model.eval()\n",
    "\n",
    "    offset = 0; valid_loss = 0\n",
    "    valid_acc=0\n",
    "    for entry in valid_batch:\n",
    "        offset+=1\n",
    "        pred = model(entry.text).squeeze(1)\n",
    "        entry.airline_sentiment = entry.airline_sentiment.type(torch.LongTensor)\n",
    "        loss = criterion(pred, entry.airline_sentiment)\n",
    "        valid_loss += loss.item()\n",
    "        pred = pred.data.max(1)[1]\n",
    "        valid_acc += float(pred.eq(entry.airline_sentiment.data).sum())\n",
    "\n",
    "    print(\"valid_loss:\", valid_loss / len(valid_batch))\n",
    "    print(\"valid_accuracy:\", valid_acc / len(valid_batch))\n",
    "    \n",
    "    if valid_acc > best_score:\n",
    "       best_score=valid_acc\n",
    "       best_model_state_dict[0] = copy.deepcopy(model.state_dict())\n",
    "       with open(\"model.pt\", 'wb') as f:\n",
    "            torch.save(best_model_state_dict[0], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch started\n",
      "---------------------------\n",
      "epoch = 0\n",
      "valid_loss: 0.8938344144821166\n",
      "valid_accuracy: 76.6\n",
      "Epoch took 1.9358348846435547 seconds\n",
      "train_loss: 0.9245321708813048\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 1\n",
      "valid_loss: 0.8653124618530273\n",
      "valid_accuracy: 77.64\n",
      "Epoch took 1.8969309329986572 seconds\n",
      "train_loss: 0.837692732350868\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 2\n",
      "valid_loss: 0.8591062521934509\n",
      "valid_accuracy: 77.88\n",
      "Epoch took 1.8548369407653809 seconds\n",
      "train_loss: 0.8227043360994574\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 3\n",
      "valid_loss: 0.8374351716041565\n",
      "valid_accuracy: 78.96\n",
      "Epoch took 1.8116416931152344 seconds\n",
      "train_loss: 0.803349050513485\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 4\n",
      "valid_loss: 0.8305393934249878\n",
      "valid_accuracy: 80.0\n",
      "Epoch took 1.8106989860534668 seconds\n",
      "train_loss: 0.7950444692059567\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 5\n",
      "valid_loss: 0.8165968751907349\n",
      "valid_accuracy: 80.08\n",
      "Epoch took 1.836101770401001 seconds\n",
      "train_loss: 0.7922742230850354\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 6\n",
      "valid_loss: 0.8073179078102112\n",
      "valid_accuracy: 80.88\n",
      "Epoch took 1.8800370693206787 seconds\n",
      "train_loss: 0.7707714009703251\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 7\n",
      "valid_loss: 0.8005557942390442\n",
      "valid_accuracy: 80.76\n",
      "Epoch took 1.844236135482788 seconds\n",
      "train_loss: 0.7579225270371688\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 8\n",
      "valid_loss: 0.7958053040504456\n",
      "valid_accuracy: 81.72\n",
      "Epoch took 1.7938358783721924 seconds\n",
      "train_loss: 0.7484575677336308\n",
      "\n",
      "epoch started\n",
      "---------------------------\n",
      "epoch = 9\n",
      "valid_loss: 0.7840125823020935\n",
      "valid_accuracy: 83.0\n",
      "Epoch took 1.8032362461090088 seconds\n",
      "train_loss: 0.74830022401977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "iteration =1\n",
    "import time\n",
    "train_loss_epoch=[]\n",
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    print(\"epoch started\")\n",
    "    print(\"---------------------------\")\n",
    "    print(\"epoch = %d\" % epoch)\n",
    "    #print(\"step_size = %.4f\" % step_size)\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for batch in train_batch:\n",
    "        iteration += 1\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred= model(batch.text).squeeze(1)\n",
    "        batch.airline_sentiment = batch.airline_sentiment.type(torch.LongTensor)\n",
    "        loss = criterion(pred, batch.airline_sentiment)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        t1 = time.time() \n",
    "    callback(epoch)\n",
    "    print(\"Epoch took {} seconds\".format(t1-t0)) \n",
    "    print(\"train_loss:\",train_loss / len(train_batch))\n",
    "    train_loss_epoch.append([epoch,train_loss / len(train_batch)])\n",
    "    print()\n",
    "    scheduler.step()\n",
    "    step_size = step_size * decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pt', 'rb') as f:\n",
    "     state_dict = torch.load(f,map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embednet(\n",
       "  (embedding): Embedding(15475, 120)\n",
       "  (fc): Linear(in_features=240, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Embednet(vocab_size, embedding_dim, n_out)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We obtain test accuracy: 84.28571428571429\n"
     ]
    }
   ],
   "source": [
    "test_acc=0.0\n",
    "for entry in test_batch:\n",
    "    pred = model(entry.text).squeeze(1)\n",
    "    entry.airline_sentiment = entry.airline_sentiment.type(torch.LongTensor)\n",
    "    pred = pred.data.max(1)[1]\n",
    "    test_acc += float(pred.eq(entry.airline_sentiment.data).sum())\n",
    "print(\" We obtain test accuracy:\",test_acc / len(test_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
